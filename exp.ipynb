{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_extractor_v2 import MultiFileFunctionExtractor\n",
    "from prompt_generator import PromptGenerator\n",
    "from call_llm import call_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function details saved to ./function_details.json\n"
     ]
    }
   ],
   "source": [
    "# file_path = \"/home/adityadev/GPTDecoder/model.py\"  # Replace with the path to your Python file\n",
    "file_paths = [\"/Users/jaylodha/Desktop/GPTDecoder/model.py\", \"/Users/jaylodha/Desktop/GPTDecoder/data_loader.py\"]  # List of Python files to process\n",
    "output_file = \"./function_details.json\"  # Output JSON file name\n",
    "\n",
    "extractor = MultiFileFunctionExtractor(file_paths, output_file)\n",
    "extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a smart Software Developer who handles large-scale Machine Learning Projects. Your task is to help the user by writing or improving code based on their input. You have access to all relevant class and method details.\n",
      "\n",
      "The user requested the following:\n",
      "\n",
      "I need help writing a function that performs a training loop using standard gradient descent. Below are my requirements:\n",
      "\n",
      "1. The model to be used is implemented in the \"GPTDecoder\" class. Please initialise the model using this class. And also refer to the \"forward\" method of the \"GPTDecoder\" class. \n",
      "2. The optimizer should be AdamW, initialized with the model parameters.\n",
      "3. For every epoch, the function should retrieve batches of data using the \"get_train_loader\" method of the \"CustomDataLoader\" class.\n",
      "4. For each batch:\n",
      "   - Pass the input batch through the forward method of the model to compute the logits and loss.\n",
      "5. Perform the backpropagation step to update the model weights.\n",
      "6. The function should print the loss value after every 100 epochs.\n",
      "\n",
      "The function should be implemented using PyTorch, and it should follow best practices for writing training loops.\n",
      "\n",
      "Note: Please return the output strictly in json format\n",
      "\n",
      "Relevant Method Details:\n",
      "**GPTDecoder.__init__**:\n",
      "Initializes the GPTDecoder module with the given model parameters.\n",
      "\n",
      "Args:\n",
      "    model_params (Dict): Dictionary containing model hyperparameters:\n",
      "        - \"vocab_size\" (int): Vocabulary size for token embeddings.\n",
      "        - \"num_embeddings\" (int): Dimensionality of embeddings.\n",
      "        - \"block_size\" (int): Maximum sequence length.\n",
      "        - \"num_heads\" (int): Number of attention heads.\n",
      "        - \"num_layers\" (int): Number of transformer blocks.\n",
      "        - \"output_classes\" (int): Number of output classes.\n",
      "        - \"dropout\" (float): Dropout probability.\n",
      "        - \"device\" (str): Device to run the model on (\"cpu\" or \"cuda\").\n",
      "**GPTDecoder.forward**:\n",
      "Performs a forward pass through the GPTDecoder.\n",
      "\n",
      "Args:\n",
      "    idx (torch.Tensor): Input token indices of shape (B, T), where\n",
      "        B = batch size,\n",
      "        T = sequence length.\n",
      "    targets (torch.Tensor, optional): Target labels of shape (B). Defaults to None.\n",
      "    mask (torch.Tensor, optional): Attention mask of shape (B, T). Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    Tuple[torch.Tensor, float]: A tuple containing:\n",
      "        - logits (torch.Tensor): Output logits of shape (B, num_classes).\n",
      "        - loss (float): Cross-entropy loss if targets are provided; otherwise, None.\n",
      "**CustomDataLoader.__init__**:\n",
      "Custom DataLoader for train and validation datasets.\n",
      "\n",
      "Args:\n",
      "    train_df: DataFrame for training data.\n",
      "    val_df: DataFrame for validation data.\n",
      "    text_col: Column index for text input.\n",
      "    label_col: Column index for labels.\n",
      "    batch_size: Batch size for loading data.\n",
      "**CustomDataLoader.get_train_loader**:\n",
      "Returns a DataLoader for the training dataset containing a batch of input_tokens, labels and corresponding mask tensor.\n",
      "\n",
      "Args:\n",
      "    shuffle (bool): Whether to shuffle the data at each epoch. Defaults to True.\n",
      "\n",
      "Returns:\n",
      "    DataLoader: DataLoader for the training dataset.\n"
     ]
    }
   ],
   "source": [
    "# Path to the centralized JSON file\n",
    "json_file = \"./function_details.json\"\n",
    "\n",
    "# Example user input\n",
    "user_prompt = \"\"\"I need help writing a function that performs a training loop using standard gradient descent. Below are my requirements:\n",
    "\n",
    "1. The model to be used is implemented in the \"GPTDecoder\" class. Please initialise the model using this class. And also refer to the \"forward\" method of the \"GPTDecoder\" class. \n",
    "2. The optimizer should be AdamW, initialized with the model parameters.\n",
    "3. For every epoch, the function should retrieve batches of data using the \"get_train_loader\" method of the \"CustomDataLoader\" class.\n",
    "4. For each batch:\n",
    "   - Pass the input batch through the forward method of the model to compute the logits and loss.\n",
    "5. Perform the backpropagation step to update the model weights.\n",
    "6. The function should print the loss value after every 100 epochs.\n",
    "\n",
    "The function should be implemented using PyTorch, and it should follow best practices for writing training loops.\n",
    "\n",
    "Note: Please return the output strictly in json format\"\"\"\n",
    "\n",
    "# Initialize the prompt generator\n",
    "prompt_generator = PromptGenerator(json_file)\n",
    "\n",
    "# Generate the final prompt\n",
    "final_prompt = prompt_generator.generate_prompt(user_prompt)\n",
    "print(final_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m out, time_taken \u001b[38;5;241m=\u001b[39m \u001b[43mcall_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Multi-file-context-LLM/call_llm.py:12\u001b[0m, in \u001b[0;36mcall_llm\u001b[0;34m(system_msg, model_name, get_json)\u001b[0m\n\u001b[1;32m      9\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.groq.com/openai/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgsk_RuIfPMqdvnRW0x0lGLfrWGdyb3FYaOmZQ0BmwHGLNEINELBcCAF0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# client = Groq(api_key=key)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel being used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/chillmate/lib/python3.12/site-packages/openai/_client.py:123\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_stream_cls \u001b[38;5;241m=\u001b[39m Stream\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletions \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mCompletions(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/chillmate/lib/python3.12/site-packages/openai/_base_client.py:857\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    845\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    846\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[1;32m    856\u001b[0m )\n\u001b[0;32m--> 857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/chillmate/lib/python3.12/site-packages/openai/_base_client.py:755\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[1;32m    754\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 755\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "model_name = \"llama-3.1-70b-versatile\"\n",
    "out, time_taken = call_llm(system_msg=final_prompt, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "out_json = json.loads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['function_definition', 'function_implementation'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "import torch\n",
      "from torch.optim import AdamW\n",
      "from torch.utils.data import DataLoader\n",
      "\n",
      "class GPTDecoder:\n",
      "    # ... (class definition)\n",
      "\n",
      "class CustomDataLoader:\n",
      "    # ... (class definition)\n",
      "\n",
      "\n",
      "def train_gpt_decoder(model_params, train_df, val_df, text_col, label_col, batch_size, num_epochs):\n",
      "    # Initialize the model\n",
      "    model = GPTDecoder(model_params)\n",
      "    device = model_params['device']\n",
      "    model.to(device)\n",
      "\n",
      "    # Initialize the optimizer\n",
      "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
      "\n",
      "    # Initialize the data loader\n",
      "    data_loader = CustomDataLoader(train_df, val_df, text_col, label_col, batch_size)\n",
      "\n",
      "    for epoch in range(num_epochs):\n",
      "        # Get the training loader for the current epoch\n",
      "        train_loader = data_loader.get_train_loader(shuffle=True)\n",
      "\n",
      "        # Iterate over the batches in the training loader\n",
      "        for batch in train_loader:\n",
      "            input_tokens, labels, mask = batch\n",
      "            input_tokens, labels, mask = input_tokens.to(device), labels.to(device), mask.to(device)\n",
      "\n",
      "            # Zero the gradients\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            # Forward pass\n",
      "            logits, loss = model(input_tokens, labels, mask)\n",
      "\n",
      "            # Backward pass\n",
      "            loss.backward()\n",
      "\n",
      "            # Update the model parameters\n",
      "            optimizer.step()\n",
      "\n",
      "        # Print the loss value after every 100 epochs\n",
      "        if (epoch + 1) % 100 == 0:\n",
      "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out_json['function_implementation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class GPTDecoder:\n",
    "    # ... (class definition)\n",
    "\n",
    "class CustomDataLoader:\n",
    "    # ... (class definition)\n",
    "\n",
    "\n",
    "def train_gpt_decoder(model_params, train_df, val_df, text_col, label_col, batch_size, num_epochs):\n",
    "    # Initialize the model\n",
    "    model = GPTDecoder(model_params)\n",
    "    device = model_params['device']\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Initialize the data loader\n",
    "    data_loader = CustomDataLoader(train_df, val_df, text_col, label_col, batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the training loader for the current epoch\n",
    "        train_loader = data_loader.get_train_loader(shuffle=True)\n",
    "\n",
    "        # Iterate over the batches in the training loader\n",
    "        for batch in train_loader:\n",
    "            input_tokens, labels, mask = batch\n",
    "            input_tokens, labels, mask = input_tokens.to(device), labels.to(device), mask.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = model(input_tokens, labels, mask)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the loss value after every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chillmate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
